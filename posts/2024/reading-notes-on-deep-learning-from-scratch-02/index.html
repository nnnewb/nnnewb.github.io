<!doctype html><html lang=zh><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="text/html; charset=UTF-8" http-equiv=content-type><meta content="width=device-width,initial-scale=1,user-scalable=no" name=viewport><meta content="index, follow" name=robots><title>《深度学习入门》读书笔记02</title><meta content=《深度学习入门》读书笔记02 name=title><meta content=一点点从这个世界上消失。 name=description><meta content=website property=og:type><meta content=https://nnnewb.github.io/posts/2024/reading-notes-on-deep-learning-from-scratch-02/ property=og:url><meta content="weakptr's blog" property=og:site_name><meta content=《深度学习入门》读书笔记02 property=og:title><meta content=一点点从这个世界上消失。 property=og:description><meta content=https://nnnewb.github.io/image/favicon.ico property=og:image><meta content=summary_large_image property=twitter:card><meta content=https://nnnewb.github.io/posts/2024/reading-notes-on-deep-learning-from-scratch-02/ property=twitter:url><meta content=《深度学习入门》读书笔记02 property=twitter:title><meta content=一点点从这个世界上消失。 property=twitter:description><meta content=https://nnnewb.github.io/image/favicon.ico property=twitter:image><link href=https://nnnewb.github.io/posts/2024/reading-notes-on-deep-learning-from-scratch-02/ rel=canonical><link rel="shortcut icon" href=https://nnnewb.github.io/image/favicon.ico type=image/x-icon><link href=https://nnnewb.github.io/css/reset.css rel=stylesheet><link href=https://nnnewb.github.io/css/pallete.css rel=stylesheet><link href=https://nnnewb.github.io/css/suCSS.css rel=stylesheet><link href=https://nnnewb.github.io/archive.css rel=stylesheet><link href=https://nnnewb.github.io/style.css rel=stylesheet><script defer src=https://nnnewb.github.io/js/script.js></script><link crossorigin href=https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css integrity=sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y rel=stylesheet><script crossorigin defer integrity=sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx src=https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js></script><script crossorigin defer integrity=sha384-zWYbd0NBwgTsgIdFKVprSfTh1mbMPe5Hz1X3yY4Sd1h/K1cQoUe36OGwAGz/PcDy src=https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/mathtex-script-type.min.js></script><script crossorigin defer integrity=sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe src=https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            // customised options
            // • auto-render specific keys, e.g.:
            delimiters: [
                { left: '$$', right: '$$', display: true },
                { left: '$', right: '$', display: false },
                { left: '\\(', right: '\\)', display: false },
                { left: '\\[', right: '\\]', display: false }
            ],
            // • rendering keys, e.g.:
            throwOnError: true
        });
    });</script><script src=https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.min.js></script><script>document.addEventListener('DOMContentLoaded', function () {
        // 查找所有具有 'pre' 标签且类名为 'language-mermaid' 的元素
        const mermaidElements = document.getElementsByClassName('language-mermaid');
        for (let i = 0; i < mermaidElements.length; i++) {
            const el = mermaidElements.item(i);
            if (el.tagName === "PRE" && !el.classList.contains('mermaid')) {
                el.innerHTML = el.textContent;
                el.classList.add('mermaid');
            }
        }

        mermaid.initialize({ startOnLoad: true, theme: 'dark', });
    })</script><script>if (window.location.hostname.toLowerCase() !== 'localhost' && window.location.hostname !== '127.0.0.1') {
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?dbb9df33a2de52aede8bccd84a7493ad";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    }</script><link href=https://chinese-fonts-cdn.deno.dev/packages/lxgwwenkaibright/dist/LXGWBright-Medium/result.css rel=stylesheet><link href=https://chinese-fonts-cdn.deno.dev/packages/maple-mono-cn/dist/MapleMono-CN-Regular/result.css rel=stylesheet><body><header><nav id=nav-bar><a href=/> 首页 </a>  /  <a href=/posts/> 文章 </a>  /  <a href=/categories/> 分类 </a>  /  <a href=/tags/> 标签 </a>  /  <a href=/search/> 搜索 </a>  /  <div><input id=theme-toggle style=display:none type=checkbox><label for=theme-toggle id=theme-toggle-label><svg class=icons id=theme-icon><use href=https://nnnewb.github.io/icons.svg#lightMode></use></svg></label><audio id=theme-sound><source src=https://nnnewb.github.io/click.ogg type=audio/ogg></audio></div></nav></header><main><h1>《深度学习入门》读书笔记02</h1><p class=author-line>作于：2024-01-05 16:50 ，预计阅读时间 6 分钟<article><h1 id=gan-zhi-ji>感知机</h1><h2 id=gan-zhi-ji-yuan-li>感知机原理</h2><blockquote><p>感知机是一种二分类模型，其核心思想是：给定一个数据集，其中每个数据点都有一个特征向量，我们希望找到一个超平面，将数据分成两类，使得超平面尽可能多地分开两个类别的数据点。<p>感知机的学习目标是找到一个超平面，使得数据点到超平面的距离最大，即找到一个分割超平面，使得正负样本尽可能分开。</blockquote><p><img alt="perceptron with two input" src=https://nnnewb.github.io/posts/2024/reading-notes-on-deep-learning-from-scratch-02/img/perceptron.webp><p>书中参考图是一个有两个输入信号 $x_1$ $x_2$ 的感知机。 $w_1$ 和 $w_2$ 是输入信号的权重(<code>w</code> 是 <code>weight</code> 的首字母)。 图中的大圆称为“神经元”或“节点”。 输入信号传入神经元时，会分别乘以固定的权重($x_{1}w_{1}$,$x_{2}w_{2}$)，神经元会计算传送过来的信号的总和。 只有当这个总和超过了某个界限值时，才会输出 1。这也称为“神经元被激活”。这里将这个界限值称为阈值，用符号 θ 表示。<p>数学表达如下：<p>$$ y=\begin{cases} 0 & (w_1x_1 + w_2x_2 \le \theta) \ 1 & (w_1x_1 + w_2x_2 \gt \theta) \end{cases} $$<p>感知机的不同输入分别对应各自的权重，权重越大表示信号的重要性越高。<h2 id=jian-dan-gan-zhi-ji-gou-zao>简单感知机构造</h2><h3 id=luo-ji-dian-lu-yu-men-huo-men-yu-fei-men>逻辑电路：与门/或门/与非门</h3><p>以两个输入的感知机为例，我们可以构造逻辑电路与/或/与非门。<p>三种门逻辑电路对应上图两个输入的感知机模型，假设输入信号取值范围是 0 或 1，实现逻辑门的权重和 $\theta$ 取值表如下。<table><thead><tr><th>gate<th>$w_1$<th>$w_2$<th>$\theta$<tbody><tr><td>and<td>1.0<td>1.0<td>$(1, 2)$<tr><td>or<td>1.0<td>1.0<td>$(0, 1)$<tr><td>not and<td>-1.0<td>-1.0<td>$(-2, -1)$</table><p>python 实现如下。<pre class=language-python data-lang=python style=color:#fdf4c1aa;background-color:#282828><code class=language-python data-lang=python><span style=color:#fa5c4b>import </span><span>numpy </span><span style=color:#fa5c4b>as </span><span>np
</span><span>
</span><span>
</span><span style=color:#fa5c4b>class </span><span style=color:#8ec07c>Perceptron</span><span>:
</span><span>    </span><span style=color:#928374;font-style:italic>"""
</span><span style=color:#928374;font-style:italic>    感知机模型
</span><span style=color:#928374;font-style:italic>    """
</span><span>
</span><span>    </span><span style=color:#fa5c4b>def </span><span style=color:#fabd2f>__init__</span><span>(</span><span style=color:#fdf4c1>self</span><span>, </span><span style=color:#fdf4c1>weights</span><span>: np.ndarray):
</span><span>        </span><span style=color:#fdf4c1>self</span><span>.weights </span><span style=color:#fe8019>= </span><span>weights
</span><span>
</span><span>    </span><span style=color:#fa5c4b>def </span><span style=color:#8ec07c>evaluate</span><span>(</span><span style=color:#fdf4c1>self</span><span>, </span><span style=color:#fdf4c1>x</span><span>: np.ndarray) -> </span><span style=color:#fabd2f>bool</span><span>:
</span><span>        </span><span style=color:#fa5c4b>return </span><span>(x</span><span style=color:#fe8019>*</span><span style=color:#fdf4c1>self</span><span>.weights)</span><span style=color:#fdf4c1>.sum()
</span><span>
</span><span>
</span><span style=color:#fabd2f>print</span><span style=color:#fdf4c1>(</span><span style=color:#b8bb26>'与门感知机，权重 [1. 1.] theta 取值区间 (1,2)'</span><span style=color:#fdf4c1>)
</span><span>p_and </span><span style=color:#fe8019>= </span><span style=color:#fdf4c1>Perceptron(np.array([</span><span style=color:#d3869b>1.</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>1.</span><span style=color:#fdf4c1>]))
</span><span>
</span><span>p_theta </span><span style=color:#fe8019>= </span><span style=color:#d3869b>1.1
</span><span>x </span><span style=color:#fe8019>= </span><span style=color:#fdf4c1>np.array([</span><span style=color:#d3869b>0.</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>0.</span><span style=color:#fdf4c1>])
</span><span>y </span><span style=color:#fe8019>= </span><span style=color:#fdf4c1>p_and.evaluate(x)
</span><span style=color:#fabd2f>print</span><span style=color:#fdf4c1>(</span><span style=color:#fa5c4b>f</span><span style=color:#b8bb26>'x=</span><span style=color:#fdf4c1>{x}</span><span style=color:#b8bb26>, theta=</span><span style=color:#fdf4c1>{p_theta}</span><span style=color:#b8bb26>, result ='</span><span style=color:#fdf4c1>, y, </span><span style=color:#b8bb26>'activated '</span><span style=color:#fdf4c1>, y </span><span style=color:#fe8019>> </span><span style=color:#fdf4c1>p_theta)
</span><span>x </span><span style=color:#fe8019>= </span><span style=color:#fdf4c1>np.array([</span><span style=color:#d3869b>1.</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>0.</span><span style=color:#fdf4c1>])
</span><span>y </span><span style=color:#fe8019>= </span><span style=color:#fdf4c1>p_and.evaluate(x)
</span><span style=color:#fabd2f>print</span><span style=color:#fdf4c1>(</span><span style=color:#fa5c4b>f</span><span style=color:#b8bb26>'x=</span><span style=color:#fdf4c1>{x}</span><span style=color:#b8bb26>, theta=</span><span style=color:#fdf4c1>{p_theta}</span><span style=color:#b8bb26>, result ='</span><span style=color:#fdf4c1>, y, </span><span style=color:#b8bb26>'activated '</span><span style=color:#fdf4c1>, y </span><span style=color:#fe8019>> </span><span style=color:#fdf4c1>p_theta)
</span><span>x </span><span style=color:#fe8019>= </span><span style=color:#fdf4c1>np.array([</span><span style=color:#d3869b>0.</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>1.</span><span style=color:#fdf4c1>])
</span><span>y </span><span style=color:#fe8019>= </span><span style=color:#fdf4c1>p_and.evaluate(x)
</span><span style=color:#fabd2f>print</span><span style=color:#fdf4c1>(</span><span style=color:#fa5c4b>f</span><span style=color:#b8bb26>'x=</span><span style=color:#fdf4c1>{x}</span><span style=color:#b8bb26>, theta=</span><span style=color:#fdf4c1>{p_theta}</span><span style=color:#b8bb26>, result ='</span><span style=color:#fdf4c1>, y, </span><span style=color:#b8bb26>'activated '</span><span style=color:#fdf4c1>, y </span><span style=color:#fe8019>> </span><span style=color:#fdf4c1>p_theta)
</span><span>x </span><span style=color:#fe8019>= </span><span style=color:#fdf4c1>np.array([</span><span style=color:#d3869b>1.</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>1.</span><span style=color:#fdf4c1>])
</span><span>y </span><span style=color:#fe8019>= </span><span style=color:#fdf4c1>p_and.evaluate(x)
</span><span style=color:#fabd2f>print</span><span style=color:#fdf4c1>(</span><span style=color:#fa5c4b>f</span><span style=color:#b8bb26>'x=</span><span style=color:#fdf4c1>{x}</span><span style=color:#b8bb26>, theta=</span><span style=color:#fdf4c1>{p_theta}</span><span style=color:#b8bb26>, result ='</span><span style=color:#fdf4c1>, y, </span><span style=color:#b8bb26>'activated '</span><span style=color:#fdf4c1>, y </span><span style=color:#fe8019>> </span><span style=color:#fdf4c1>p_theta)
</span></code></pre><pre style=color:#fdf4c1aa;background-color:#282828><code><span>与门感知机，权重 [1. 1.] theta 取值区间 (1,2)
</span><span>x=[0. 0.], theta=1.1, result = 0.0 activated  False
</span><span>x=[1. 0.], theta=1.1, result = 1.0 activated  False
</span><span>x=[0. 1.], theta=1.1, result = 1.0 activated  False
</span><span>x=[1. 1.], theta=1.1, result = 2.0 activated  True
</span></code></pre><h2 id=pian-zhi-he-theta>偏置和 theta</h2><p>如果说权重表示输入的重要程度，偏置项(bias)就表示神经元被激活的容易程度。按感知机的定义，偏置项越高，则神经元越容易激活。<p>将偏置项 $b$ 定义为 $-\theta$ 则有下面的感知机模型。<p>$$ y=\begin{cases} 0 & (w_1x_1 + w_2x_2 + b \le 0) \ 1 & (w_1x_1 + w_2x_2 + b \gt 0) \end{cases} $$<p>可以看成 $\theta$ 为$0$的感知机模型，同时增加了一个偏置项。<p>python 定义如下。<pre class=language-python data-lang=python style=color:#fdf4c1aa;background-color:#282828><code class=language-python data-lang=python><span style=color:#fa5c4b>class </span><span style=color:#8ec07c>Perceptron</span><span>:
</span><span>    </span><span style=color:#928374;font-style:italic>"""
</span><span style=color:#928374;font-style:italic>    感知机模型
</span><span style=color:#928374;font-style:italic>    """
</span><span>
</span><span>    </span><span style=color:#fa5c4b>def </span><span style=color:#fabd2f>__init__</span><span>(</span><span style=color:#fdf4c1>self</span><span>, </span><span style=color:#fdf4c1>weights</span><span>: np.ndarray, </span><span style=color:#fdf4c1>bias</span><span>: </span><span style=color:#fabd2f>float</span><span>):
</span><span>        </span><span style=color:#fdf4c1>self</span><span>.weights </span><span style=color:#fe8019>= </span><span>weights
</span><span>        </span><span style=color:#fdf4c1>self</span><span>.bias </span><span style=color:#fe8019>= </span><span>bias
</span><span>
</span><span>    </span><span style=color:#fa5c4b>def </span><span style=color:#8ec07c>evaluate</span><span>(</span><span style=color:#fdf4c1>self</span><span>, </span><span style=color:#fdf4c1>x</span><span>: np.ndarray) -> </span><span style=color:#fabd2f>bool</span><span>:
</span><span>        </span><span style=color:#fa5c4b>return </span><span style=color:#fdf4c1>np.sum(x</span><span style=color:#fe8019>*</span><span style=color:#fdf4c1>self.weights)</span><span style=color:#fe8019>+</span><span style=color:#fdf4c1>self</span><span>.bias </span><span style=color:#fe8019>> </span><span style=color:#d3869b>0
</span></code></pre><h2 id=gan-zhi-ji-de-ju-xian>感知机的局限</h2><p>这里需要一点高中数学知识。<p>以与门感知机为例，简化起见，权重设为 1，偏置设为-2.0，将权重和偏置代入感知机定义公式。<p>$$ y=\begin{cases} 0 & (x_1 + x_2 - 2.0 \le 0) \ 1 & (x_1 + x_2 - 2.0 \gt 0) \end{cases} $$<p>把不等式方程组画出图像，会发现是一个被 $x_1 + x_2 - 2.0 = 0$ 这条直线分割的两块空间。一块空间表示真值一块表示假。<pre class=language-python data-lang=python style=color:#fdf4c1aa;background-color:#282828><code class=language-python data-lang=python><span style=color:#fa5c4b>import </span><span>numpy </span><span style=color:#fa5c4b>as </span><span>np
</span><span style=color:#fa5c4b>import </span><span>matplotlib.pyplot </span><span style=color:#fa5c4b>as </span><span>plt
</span><span>
</span><span>x1 </span><span style=color:#fe8019>= </span><span style=color:#fdf4c1>np.arange(</span><span style=color:#fe8019>-</span><span style=color:#d3869b>3</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>3</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>0.1</span><span style=color:#fdf4c1>)
</span><span>x2 </span><span style=color:#fe8019>= </span><span style=color:#d3869b>2.0</span><span style=color:#fe8019>-</span><span>x1
</span><span>
</span><span style=color:#fdf4c1>plt.plot([</span><span style=color:#d3869b>0</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>2</span><span style=color:#fdf4c1>], [</span><span style=color:#d3869b>2</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>0</span><span style=color:#fdf4c1>], </span><span style=color:#b8bb26>'go'</span><span style=color:#fdf4c1>, markersize</span><span style=color:#fe8019>=</span><span style=color:#d3869b>5</span><span style=color:#fdf4c1>)
</span><span style=color:#fdf4c1>plt.fill([</span><span style=color:#fe8019>-</span><span style=color:#d3869b>3</span><span style=color:#fdf4c1>, </span><span style=color:#fe8019>-</span><span style=color:#d3869b>3</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>3</span><span style=color:#fdf4c1>, </span><span style=color:#fe8019>-</span><span style=color:#d3869b>3</span><span style=color:#fdf4c1>], [</span><span style=color:#d3869b>5</span><span style=color:#fdf4c1>, </span><span style=color:#fe8019>-</span><span style=color:#d3869b>1</span><span style=color:#fdf4c1>, </span><span style=color:#fe8019>-</span><span style=color:#d3869b>1</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>5</span><span style=color:#fdf4c1>], </span><span style=color:#b8bb26>'red'</span><span style=color:#fdf4c1>, alpha</span><span style=color:#fe8019>=</span><span style=color:#d3869b>0.5</span><span style=color:#fdf4c1>)
</span><span style=color:#fdf4c1>plt.plot(x1, x2)
</span><span style=color:#fdf4c1>plt.axhline(y</span><span style=color:#fe8019>=</span><span style=color:#d3869b>0</span><span style=color:#fdf4c1>, color</span><span style=color:#fe8019>=</span><span style=color:#b8bb26>'k'</span><span style=color:#fdf4c1>)
</span><span style=color:#fdf4c1>plt.axvline(x</span><span style=color:#fe8019>=</span><span style=color:#d3869b>0</span><span style=color:#fdf4c1>, color</span><span style=color:#fe8019>=</span><span style=color:#b8bb26>'k'</span><span style=color:#fdf4c1>)
</span><span style=color:#fdf4c1>plt.grid(</span><span style=color:#d3869b>True</span><span style=color:#fdf4c1>, axis</span><span style=color:#fe8019>=</span><span style=color:#b8bb26>'both'</span><span style=color:#fdf4c1>,linestyle</span><span style=color:#fe8019>=</span><span style=color:#b8bb26>':'</span><span style=color:#fdf4c1>)
</span><span style=color:#fdf4c1>plt.show()
</span></code></pre><p><img alt=png src=https://nnnewb.github.io/posts/2024/reading-notes-on-deep-learning-from-scratch-02/index_files/index_3_0.webp><p>可以直观地看到，当输入的 $x_1$、$x_2$ 构成的点落在图中红色区域内，感知机输出假。反之输出真。<p>以与门为例。<pre class=language-python data-lang=python style=color:#fdf4c1aa;background-color:#282828><code class=language-python data-lang=python><span style=color:#fa5c4b>import </span><span>numpy </span><span style=color:#fa5c4b>as </span><span>np
</span><span style=color:#fa5c4b>import </span><span>matplotlib.pyplot </span><span style=color:#fa5c4b>as </span><span>plt
</span><span>
</span><span>x1 </span><span style=color:#fe8019>= </span><span style=color:#fdf4c1>np.arange(</span><span style=color:#fe8019>-</span><span style=color:#d3869b>3</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>3</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>0.1</span><span style=color:#fdf4c1>)
</span><span>x2 </span><span style=color:#fe8019>= </span><span style=color:#d3869b>1.1</span><span style=color:#fe8019>-</span><span>x1
</span><span>
</span><span style=color:#fdf4c1>plt.plot(x1, x2)
</span><span style=color:#fdf4c1>plt.plot(
</span><span style=color:#fdf4c1>    np.array([</span><span style=color:#d3869b>0.</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>0.</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>1.</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>1.</span><span style=color:#fdf4c1>]),
</span><span style=color:#fdf4c1>    np.array([</span><span style=color:#d3869b>1.</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>0.</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>1.</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>0.</span><span style=color:#fdf4c1>]),
</span><span style=color:#fdf4c1>    </span><span style=color:#b8bb26>'go'</span><span style=color:#fdf4c1>)
</span><span style=color:#fdf4c1>plt.axhline(y</span><span style=color:#fe8019>=</span><span style=color:#d3869b>0</span><span style=color:#fdf4c1>, color</span><span style=color:#fe8019>=</span><span style=color:#b8bb26>'k'</span><span style=color:#fdf4c1>)
</span><span style=color:#fdf4c1>plt.axvline(x</span><span style=color:#fe8019>=</span><span style=color:#d3869b>0</span><span style=color:#fdf4c1>, color</span><span style=color:#fe8019>=</span><span style=color:#b8bb26>'k'</span><span style=color:#fdf4c1>)
</span><span style=color:#fdf4c1>plt.grid(</span><span style=color:#d3869b>True</span><span style=color:#fdf4c1>, axis</span><span style=color:#fe8019>=</span><span style=color:#b8bb26>'both'</span><span style=color:#fdf4c1>, linestyle</span><span style=color:#fe8019>=</span><span style=color:#b8bb26>':'</span><span style=color:#fdf4c1>)
</span><span style=color:#fdf4c1>plt.show()
</span></code></pre><p><img alt=png src=https://nnnewb.github.io/posts/2024/reading-notes-on-deep-learning-from-scratch-02/index_files/index_5_0.webp><p>当图中 $1&lt;\theta&lt;2$ 且有 $x_1=1,x_2=1$，蓝色点才会落到白色区域。而直线上的点按我们的感知机定义，会视为假值。<p>当我们修改偏置值时，这条直线就会上下移动。<ul><li>当 $1&lt;\theta&lt;2$ 时，感知机就变成了与逻辑门，(1,1)被视为真值，(1,0)和(0,1)都被视为假值。<li>当 $0&lt;\theta&lt;1$ 时，则感知机就变成了或逻辑门，(1,0)和(0,1)都被视为真值。<li>当 $-2&lt;\theta&lt; -1$ 时且权值 $w_1=-1,w_2=-1$，图形中红色区域和白色区域反转，(0,0),(0,1),(1,0) 视为真值，直线位置和 $1&lt;\theta&lt;2$ 范围一致。</ul><p>当需要分类的输入组合是 (0,0),(1,1) 和 (1,0),(0,1) 时，感知机就无法正确分类了。<p>对于这种情况，可以引入多层感知机。<h2 id=duo-ceng-gan-zhi-ji>多层感知机</h2><p>多层感知机就是多个感知机的堆叠。<p>读过数电应该知道区分 (0,0)(1,1) 和 (1,0)(0,1) 其实就是实现一个异或门，不同时输出 1，相同时输出 0。<p>而异或门本身可以由或门、与非门和与门组成。<p><img alt=perceptron src=https://nnnewb.github.io/posts/2024/reading-notes-on-deep-learning-from-scratch-02/img/perceptron-fig3.webp><p>这里用 python 构建出多层感知机。<pre class=language-python data-lang=python style=color:#fdf4c1aa;background-color:#282828><code class=language-python data-lang=python><span style=color:#fa5c4b>class </span><span style=color:#8ec07c>Perceptron</span><span>:
</span><span>    </span><span style=color:#928374;font-style:italic>"""
</span><span style=color:#928374;font-style:italic>    感知机模型
</span><span style=color:#928374;font-style:italic>    """
</span><span>
</span><span>    </span><span style=color:#fa5c4b>def </span><span style=color:#fabd2f>__init__</span><span>(</span><span style=color:#fdf4c1>self</span><span>, </span><span style=color:#fdf4c1>weights</span><span>: np.ndarray, </span><span style=color:#fdf4c1>bias</span><span>: </span><span style=color:#fabd2f>float</span><span>):
</span><span>        </span><span style=color:#fdf4c1>self</span><span>.weights </span><span style=color:#fe8019>= </span><span>weights
</span><span>        </span><span style=color:#fdf4c1>self</span><span>.bias </span><span style=color:#fe8019>= </span><span>bias
</span><span>
</span><span>    </span><span style=color:#fa5c4b>def </span><span style=color:#8ec07c>evaluate</span><span>(</span><span style=color:#fdf4c1>self</span><span>, </span><span style=color:#fdf4c1>x</span><span>: np.ndarray) -> </span><span style=color:#fabd2f>bool</span><span>:
</span><span>        </span><span style=color:#fa5c4b>return </span><span style=color:#fdf4c1>np.sum(x</span><span style=color:#fe8019>*</span><span style=color:#fdf4c1>self.weights)</span><span style=color:#fe8019>+</span><span style=color:#fdf4c1>self</span><span>.bias </span><span style=color:#fe8019>> </span><span style=color:#d3869b>0
</span><span>
</span><span>
</span><span style=color:#fa5c4b>def </span><span style=color:#8ec07c>xor</span><span>(</span><span style=color:#fdf4c1>x1</span><span>: </span><span style=color:#fabd2f>float</span><span>, </span><span style=color:#fdf4c1>x2</span><span>: </span><span style=color:#fabd2f>float</span><span>) -> </span><span style=color:#fabd2f>bool</span><span>:
</span><span>    p_nand </span><span style=color:#fe8019>= </span><span style=color:#fdf4c1>Perceptron(np.array([</span><span style=color:#fe8019>-</span><span style=color:#d3869b>1.0</span><span style=color:#fdf4c1>, </span><span style=color:#fe8019>-</span><span style=color:#d3869b>1.0</span><span style=color:#fdf4c1>]), </span><span style=color:#d3869b>1.5</span><span style=color:#fdf4c1>)
</span><span>    p_or </span><span style=color:#fe8019>= </span><span style=color:#fdf4c1>Perceptron(np.array([</span><span style=color:#d3869b>1.0</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>1.0</span><span style=color:#fdf4c1>]), </span><span style=color:#fe8019>-</span><span style=color:#d3869b>0.5</span><span style=color:#fdf4c1>)
</span><span>    p_and </span><span style=color:#fe8019>= </span><span style=color:#fdf4c1>Perceptron(np.array([</span><span style=color:#d3869b>1.0</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>1.0</span><span style=color:#fdf4c1>]), </span><span style=color:#fe8019>-</span><span style=color:#d3869b>1.5</span><span style=color:#fdf4c1>)
</span><span>
</span><span>    s1 </span><span style=color:#fe8019>= </span><span style=color:#d3869b>1.0 </span><span style=color:#fa5c4b>if </span><span style=color:#fdf4c1>p_nand.evaluate(np.array([x1, x2])) </span><span style=color:#fa5c4b>else </span><span style=color:#d3869b>0.0
</span><span>    s2 </span><span style=color:#fe8019>= </span><span style=color:#d3869b>1.0 </span><span style=color:#fa5c4b>if </span><span style=color:#fdf4c1>p_or.evaluate(np.array([x1, x2])) </span><span style=color:#fa5c4b>else </span><span style=color:#d3869b>0.0
</span><span>    </span><span style=color:#fa5c4b>return </span><span style=color:#fdf4c1>p_and.evaluate(np.array([s1, s2]))
</span><span>
</span><span>
</span><span style=color:#fabd2f>print</span><span style=color:#fdf4c1>(</span><span style=color:#b8bb26>'xor(0.0, 0.0)='</span><span style=color:#fdf4c1>, xor(</span><span style=color:#d3869b>0.0</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>0.0</span><span style=color:#fdf4c1>))
</span><span style=color:#fabd2f>print</span><span style=color:#fdf4c1>(</span><span style=color:#b8bb26>'xor(0.0, 1.0)='</span><span style=color:#fdf4c1>, xor(</span><span style=color:#d3869b>0.0</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>1.0</span><span style=color:#fdf4c1>))
</span><span style=color:#fabd2f>print</span><span style=color:#fdf4c1>(</span><span style=color:#b8bb26>'xor(1.0, 0.0)='</span><span style=color:#fdf4c1>, xor(</span><span style=color:#d3869b>1.0</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>0.0</span><span style=color:#fdf4c1>))
</span><span style=color:#fabd2f>print</span><span style=color:#fdf4c1>(</span><span style=color:#b8bb26>'xor(1.0, 1.0)='</span><span style=color:#fdf4c1>, xor(</span><span style=color:#d3869b>1.0</span><span style=color:#fdf4c1>, </span><span style=color:#d3869b>1.0</span><span style=color:#fdf4c1>))
</span></code></pre><pre style=color:#fdf4c1aa;background-color:#282828><code><span>xor(0.0, 0.0)= False
</span><span>xor(0.0, 1.0)= True
</span><span>xor(1.0, 0.0)= True
</span><span>xor(1.0, 1.0)= False
</span></code></pre><h2 id=wei-sheng>尾声</h2><p>读完这一章有几个疑问：<ol><li>多层感知机能表示表示非线性空间，能给多层感知机画出函数图像吗？长什么样？<li>如果给两个样本集合，怎么判断这个两个集合能不能被单层感知机分类？</ol></article><p class=tags-data><a href=/tags/python>/python/</a> <a href=/tags/numpy>/numpy/</a> <a href=/tags/matplotlib>/matplotlib/</a> <a href=/tags/gan-zhi-ji>/感知机/</a> <a href=/tags/shen-du-xue-xi>/深度学习/</a></p><script data-repo-id="MDEwOlJlcG9zaXRvcnkzOTg0ODYyMTg=" async crossorigin data-category=Announcements data-category-id=DIC_kwDOF8Bqys4Cegmn data-emit-metadata=0 data-input-position=bottom data-lang=zh-CN data-mapping=pathname data-reactions-enabled=1 data-repo=nnnewb/nnnewb.github.io data-strict=0 data-theme=noborder_light id=giscus_script src=https://giscus.app/client.js></script></main><footer><hr><div id=footer-container><div><p style=text-align:center>Copyright © 2018-2024 weakptr <a href=mailto:weak_ptr@outlook.com>&lt;weak_ptr@outlook.com></a><p style=text-align:center>Built with <a rel="noopener noreferrer" href=https://www.getzola.org target=_blank>Zola</a> using <a rel="noopener noreferrer" href=https://github.com/Speyll/anemone target=_blank>anemone</a> theme, <a rel="noopener noreferrer" href=https://speyll.github.io/suCSS/ target=_blank>suCSS</a> framework & <a rel="noopener noreferrer" href=https://github.com/Speyll/veqev target=_blank>veqev</a>, modified by <a rel="noopener noreferrer" href=https://github.com/nnnewb/ target=_blank>nnnewb</a>.<p style=text-align:center>Theme and color theme licensed under <a rel="noopener noreferrer" href=https://en.wikipedia.org/wiki/Licence_MIT target=_blank>MIT</a>.</div></div></footer>